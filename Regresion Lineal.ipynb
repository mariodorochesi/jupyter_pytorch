{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los algoritmos basicos sobre el cual funciona la rama de machine learning es la *Regresion Lineal*. Como modo introductorio a Pytorch vamos a crear un modelo que sea capaz de predecir la produccion de manzanas y naranjas (variables objetivo) de diferentes regiones en base a la temperatura promedio, precipitaciones y la humedad (variables de entrada) en una region. Veamos la data de entranamiento.\n",
    "\n",
    "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
    "\n",
    "En un modelo de regresion lineal cada variable objetivo es estimada como la suma de un peso por las variables de entrada mas una constante de sesgo conocida como bias.\n",
    "\n",
    "\n",
    "```\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data de entrenamiento\n",
    "\n",
    "Si nos fijamos en la data de entrenamiento podriamos representarla como dos matrices: `entradas` y `objetivos`, con una fila por cada region y una columna por cada variable de la observacion. Para el caso de la tabla de arriba tendriamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada (temp, rainfall, humidity)\n",
    "inputs = np.array([\n",
    "    [73,67,43],\n",
    "    [91,88,64],\n",
    "    [87,134,58],\n",
    "    [102,43,37],\n",
    "    [69,96,70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetivos = np.array([\n",
    "    [56,70],\n",
    "    [81,101],\n",
    "    [119,133],\n",
    "    [22,37],\n",
    "    [103,119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, hemos separado la data de entrenamiento en dos matrices debido a que vamos a necesitar operar con ellas de forma independiente. Se han creado como arreglo de Numpy debido a que tipicamente cualquier dataset que encontremos pueede ser leido en formato CSV directamente a arreglos de Numpy. **Entonces ahora debemos transformarlo a tensors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Convertimos los arreglos de entrada a tensores\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(objetivos)\n",
    "print(f'{inputs}')\n",
    "print(f'{targets}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de regresion lineal\n",
    "\n",
    "Los pesos y los sesgos (`w11, w12, ... w23, b1 & b2`) pueden ser presentados como matrices inicializadas con valores aleatorios. La primera fila de `w` y el primer elemento de `b` son usados para predecir la primera variable objetivo, por ejemplo la produccion de manzanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6275,  0.2851, -0.3010],\n",
      "        [-0.0155, -0.3856, -0.1424]], requires_grad=True)\n",
      "tensor([ 0.1501, -0.9704], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Se inicializan pesos y sesgos\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad = True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.randn` create un tensor de la formada dada con sus elementos escogidos aleatoriamente con una distribucion normal con media 0 y desviacion estandar 1.\n",
    "\n",
    "El modelo para resolver el problema es simplemente una funcion que realiza una multiplicacion de matrices de entrada `inputs` y los pesos `w`(traspuestos) y luego le suma el sesgo `b`.\n",
    "\n",
    "![matrix-mult](https://i.imgur.com/WGXLFvA.png)\n",
    "\n",
    "**Podemos definir el modelo en codigo de la siguiente forma**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@` representa una multiplicacion de matrices en Pytorch. El metodo `.t()` retorna la traspuesta de un tensor.\n",
    "\n",
    "La matriz obtenida al pasar la data de entrada a nuestro moduelo genera una prediccion de las variables objetivos como se puede ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-39.4947, -34.0628],\n",
      "        [-51.1223, -45.4304],\n",
      "        [-33.6924, -62.2528],\n",
      "        [-62.7274, -24.4036],\n",
      "        [-36.8433, -49.0282]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Genera predicciones\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos la prediccion de nuestro modelo con los objetivos reales de la data de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver existe una gran diferencia entre las predicciones de nuestro modelos y los valores reales de las variables objetivo. Obviamente, esto se debe a que hemos inicializado los pesos y sesgos de nustro modelo con variables aleatorias. No podemos esperar que simplemente funcione asi.\n",
    "\n",
    "# Funcion de perdida\n",
    "\n",
    "Antes de mejorar nuestro modelo necesitamos un mecanismo para poder evaluar que tan bien o mal esta aproximando nuestro modelo comparado con los objetivos reales, para ello vamos a calcular el error cuadratico medio **MSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(t1,t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff*diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo `torch.sum` retorna la suma de todos los elementos en un tensor, y el metodo `.numel` retorna la cantidad de elementos en un tensor. Vamos a calcular el error cuadrado medio para la prediccion actual de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17902.3887, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso el error dio 15344. Esto se puede interpretar como que cada elemento difiere del objetivo real por 123 unidades (raiz cuadrada de 15344). Claramente el error esta demasiado alto, por lo cual debemos buscar una forma de reducirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular gradientes\n",
    "\n",
    "Con Pytorch podemos automaticamente calcular el gradiente o la derivada de la funcion de perdida con respecto a los pesos y el sesgo debido a que `requires_grad` esta seteado como `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular gradientes\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los gradientes se almacenan en el atributo `.grad` del tensor respectivo. En este caso la derivada con respecto a los pesos es en si una matriz con las mismas dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6275,  0.2851, -0.3010],\n",
      "        [-0.0155, -0.3856, -0.1424]], requires_grad=True)\n",
      "tensor([[-10113.9746, -11110.7871,  -6868.4419],\n",
      "        [-11153.1748, -12958.6055,  -7840.9648]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion de perdida es una funcion cuadratica de los pesos y sesgos. Nuestro objetivo es encontrar un conjunto de pesos y bias para los cuales la funcion alcance su minimo posible. Si graficamos la funcion de perdida con respecto a cualquier peso individual o sesgo nos encontraremos como una figura como la siguiente.\n",
    "\n",
    "![postive-gradient](https://i.imgur.com/hFYoVgU.png)\n",
    "\n",
    "En donde, si el **gradiente del elemento es positivo** entonces disminuir ligeramente el valor del elemento disminuira la perdida. \n",
    "\n",
    "Por otro lado podemos encontrar casos como:\n",
    "\n",
    "![negative=gradient](https://i.imgur.com/w3Wii7C.png)\n",
    "\n",
    "En donde si el **gradiente del elemento es negativo** entonces aumentar ligeramente el valor del elemento disminuira la perdida.\n",
    "\n",
    "\n",
    "Aumentar o disminuir la funcion de perdida al cambiar los pesos de un elemento proporcional a su gradiente es la base del algoritmo de optimizacion que vamos a usar para mejorar nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de proceder vamos a resetear los gradientes de los tensors a cero. Para ello vamos a llamar al metodo `.zero()`.**Necesitamos hacer esto debido a que Pytorch acumula los gradientes**. Por ejemplo la proxima vez que llamemos al metodo `.backward` de la funcion de perdida, los nuevos valores del gradiente seran sumados con los valores existentes lo cual podria llevar a resultados inesperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustar los pesos y el sesgo utilizando descenso de gradiente\n",
    "\n",
    "Ahora vamos a reducir el riesgo y mejorar nuestro modelo utilizando la optimizacion por decsenso de gradiente siguiendo los siguientes pasos:\n",
    "\n",
    "1. Generar la prediccion.\n",
    "2. Calcular la perdida.\n",
    "3. Calcular los gradientes con respecto a los pesos y el sesgo.\n",
    "4. Ajustar los pesos al restarles una pequeña proporcion del gradiente.\n",
    "5. Resetear los gradientes a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-39.4947, -34.0628],\n",
      "        [-51.1223, -45.4304],\n",
      "        [-33.6924, -62.2528],\n",
      "        [-62.7274, -24.4036],\n",
      "        [-36.8433, -49.0282]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generar las predicciones\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17902.3887, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calcular la perdida\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-10113.9746, -11110.7871,  -6868.4419],\n",
      "        [-11153.1748, -12958.6055,  -7840.9648]])\n",
      "tensor([-120.9760, -135.0356])\n"
     ]
    }
   ],
   "source": [
    "# Calcular gradientes\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar pesos y sesgos\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se usa `torch.no_grad` para indicarle a Pytorch que no deberia hacer un seguimiento, calculo o modificacion de los gradientes luego de actualizar los pesos y sesgos.\n",
    "- Se multiplican los gradientes por un numero muy pequeño, para asegurarnos de no hacer un cambio muy brusco ya que queremos aproximarnos poco a poco a la solucion. **Esto se conoce como tasa de aprendizaje**.\n",
    "- Despues de actualizar los pesos resetiamos los gradientes a cero para evitar afectar calculos futuros.\n",
    "\n",
    "Veamos como se ven los pesos y sesgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5263,  0.3962, -0.2323],\n",
      "        [ 0.0960, -0.2560, -0.0640]], requires_grad=True)\n",
      "tensor([ 0.1513, -0.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos pesos y sesgos, el modelo disminuye el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12195.7051, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente hemos logrado disminuir significativamente la funcion de perdida. Ahora solo bastaria realizar este proceso iterativamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrando el modelo\n",
    "\n",
    "Para reducir aun mas la funcion de perdida, lo que debemos hacer ahora es iterar multiples veces. **Cada iteracion es lo que conocemos como una epoca**. En este caso vamos a entrenar el modelo por 1000 epocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds,targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora verificamos que la funcion de perdida sea menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1004, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, la funcion de perdida ha disminuido considerablemente. Ahora vamos a ver las predicciones del modelo y compararlas con los objetivos de la data de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.4012,  70.3532],\n",
       "        [ 80.4624,  99.1743],\n",
       "        [122.2816, 136.2866],\n",
       "        [ 22.1098,  38.0585],\n",
       "        [ 98.3163, 115.7360]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, la prediccion es muy cercana a las variables objetivo. Se podrian obtener aun mejores resultados entrenando el modelo por mas epocas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
