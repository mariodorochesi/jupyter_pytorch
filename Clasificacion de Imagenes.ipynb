{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se va a utilizar Pytorch para poder clasificar el datase MNIST de dígitos escritos a mano. Consiste en imagenes de 28px x 28px indicando digitos escritos a mano del 0 al 9.\n",
    "\n",
    "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfec706161f245c494d1263c3e6f96e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9d8e466bfe4632afb7f47ea257cd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa13a3a771574ae692b15f8753eb92cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ef0de07794433ca790aaee46d465ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/godorluis/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Descargar el dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada dato del arreglo del dataset consiste en una imagen de 28x28 pixeles y su respectivo label. Podemos visualizar la imagen utilizando la libreria matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F5338C4C910>, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(f'Label : {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, es evidente que algunas imágenes son dificiles de reconocer incluso para el ojo humano. Si bien es útil poder ver la imagen tenemos un problema aquí. **Pytorch no sabe como trabajar con imágenes**. Necesitamos convertir las imagenes a tensores. Se puede lograr al aplicar una transformacion incluida en Pytorch al crear el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datasets de Pytorch nos permiten especificar una o mas funciones de transformacion que son aplicadas a las imagenes mientras son cargadas. `torchvision.transforms` contiene muchas funciones predefinidas, en este caso ocuparemos la funcion `ToTensor` para convertir las imagenes en tensores de Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(\n",
    "    root='data/',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen es transformada a un tensor de 1x28x28. La primera dimension es usada para mantener un registro de los canales de color. Como las imagenes del dataset MNIST estan en escala de grises hay solo un canal. Otros datasets tienen imagenes con color en cuyo caso hay 3 canales: rojo, verde y azul. Veamos aljunos ejemplos de los valores dentro del tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:,10:15,10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores van en un rango de 0 a 1 con 0 representando negro y 1 representando blanco. Los valores entremedio representan diferentes niveles de grises. Podemos plotear tambien como una imagen el tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJMElEQVR4nO3d34uUhR7H8c/nrEZRB7qwi3BFIyKQ4BSIBF4EQmQWdVtg3VR7cwKDIOqyfyC66WapSEiMoC6iOoSQEUFWW22SWWA/DhmB5yBa3RTmp4sZDh7ZdZ8Z55lnni/vFyzs7AwzH2TfPjOzy7NOIgB1/K3rAQAmi6iBYogaKIaogWKIGihmXRt3ars3b6lv3ry56wkj2bBhQ9cTRvL99993PaGxU6dOdT1hJEm80tfdxo+0bMde8fFmzuLiYtcTRvLwww93PWEke/bs6XpCY/v37+96wkhWi5qn30AxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDGNora9y/Y3to/bfrLtUQDGt2bUtuckPSfpTklbJd1ve2vbwwCMp8mReruk40m+S/KHpFck3dvuLADjahL1Rkk/nnf5xPBr/8f2gu0l20uTGgdgdBM7RXCSRUmLUr9OEQxU0+RI/ZOkTeddnh9+DcAMahL1J5JusH2d7csk3SfpjXZnARjXmk+/k5y1/aikdyTNSXoxydHWlwEYS6PX1EnelvR2y1sATAC/UQYUQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDETO/HghZJ+nHvwzJkzXU8o7ZFHHul6QmMHDhzoekJj586dW/U6jtRAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxa0Zt+0XbJ21/OY1BAC5NkyP1S5J2tbwDwISsGXWS9yWdmsIWABPAa2qgmImdTdT2gqSFSd0fgPFMLOoki5IWJcl2P84PDBTE02+gmCY/0jog6UNJN9o+Yfuh9mcBGNeaT7+T3D+NIQAmg6ffQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U42TypxPr0znKrrzyyq4njOStt97qesJIbrvttq4nNHbHHXd0PaGxw4cP68yZM17pOo7UQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFLNm1LY32T5k+yvbR23vncYwAONZ1+A2ZyU9nuQz23+X9Kntg0m+ankbgDGseaRO8nOSz4af/yrpmKSNbQ8DMJ4mR+r/sb1F0i2SPlrhugVJC5OZBWBcjaO2fZWk1yQ9luSXC69PsihpcXjb3pwiGKim0bvfttdrEPT+JK+3OwnApWjy7rclvSDpWJJn2p8E4FI0OVLvkPSApJ22l4cfu1veBWBMa76mTvKBpBX/vAeA2cNvlAHFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UIyTyZ8jkBMPtuf666/vesJIlpeXu57Q2OnTp7ue0Nju3bt15MiRFU9ewpEaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooZs2obV9u+2PbX9g+avvpaQwDMJ51DW7zu6SdSX6zvV7SB7b/leRwy9sAjGHNqDM4idlvw4vrhx+cgwyYUY1eU9ues70s6aSkg0k+anUVgLE1ijrJn0luljQvabvtmy68je0F20u2lya8EcAIRnr3O8lpSYck7VrhusUk25Jsm9A2AGNo8u73NbavHn5+haTbJX3d8i4AY2ry7ve1kvbZntPgP4FXk7zZ7iwA42ry7vcRSbdMYQuACeA3yoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKbJmU8wQ7799tuuJ4zkwQcf7HpCY/v27et6QmPr1q2eLkdqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGimkcte0525/bfrPNQQAuzShH6r2SjrU1BMBkNIra9rykuyQ93+4cAJeq6ZH6WUlPSDq32g1sL9hesr00iWEAxrNm1LbvlnQyyacXu12SxSTbkmyb2DoAI2typN4h6R7bP0h6RdJO2y+3ugrA2NaMOslTSeaTbJF0n6R3k+xpfRmAsfBzaqCYkf7sTpL3JL3XyhIAE8GRGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYpxk8ndq/0fSvyd8txsk/XfC99mmPu3t01apX3vb2ro5yTUrXdFK1G2wvdSnM5X2aW+ftkr92tvFVp5+A8UQNVBMn6Je7HrAiPq0t09bpX7tnfrW3rymBtBMn47UABogaqCYXkRte5ftb2wft/1k13suxvaLtk/a/rLrLWuxvcn2Idtf2T5qe2/Xm1Zj+3LbH9v+Yrj16a43NWF7zvbntt+c1mPOfNS25yQ9J+lOSVsl3W97a7erLuolSbu6HtHQWUmPJ9kq6VZJ/5zhf9vfJe1M8g9JN0vaZfvWbic1slfSsWk+4MxHLWm7pONJvkvyhwZ/efPejjetKsn7kk51vaOJJD8n+Wz4+a8afPNt7HbVyjLw2/Di+uHHTL/La3te0l2Snp/m4/Yh6o2Sfjzv8gnN6Dden9neIukWSR91PGVVw6eyy5JOSjqYZGa3Dj0r6QlJ56b5oH2IGi2zfZWk1yQ9luSXrvesJsmfSW6WNC9pu+2bOp60Ktt3SzqZ5NNpP3Yfov5J0qbzLs8Pv4YJsL1eg6D3J3m96z1NJDkt6ZBm+72LHZLusf2DBi8Zd9p+eRoP3IeoP5F0g+3rbF+mwR++f6PjTSXYtqQXJB1L8kzXey7G9jW2rx5+foWk2yV93emoi0jyVJL5JFs0+J59N8meaTz2zEed5KykRyW9o8EbOa8mOdrtqtXZPiDpQ0k32j5h+6GuN13EDkkPaHAUWR5+7O561CqulXTI9hEN/qM/mGRqPybqE35NFChm5o/UAEZD1EAxRA0UQ9RAMUQNFEPUQDFEDRTzF4427ALf1TFUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets de entrenamiento y validación\n",
    "\n",
    "Mientra se crean modelos de machine learning en la práctica, es bastante común dividir el dataset en 3 partes:\n",
    "\n",
    "1. *Set de entrenamiento*: Usado para entrenar el modulo, por ejemplo para calcular la perdida y ajustar los pesos del modelo usando el descenso de gradiente.\n",
    "2. *Set de validación*: Usado para evaluar el modelo mientras se entrena, ajustar hiperparámetros (como la tasa de aprendizaje) y escoger la mejor versión del modelo.\n",
    "3. *Set de testing*: Usado para comparar diferentes modelos, y reportar finalmente la precisión del modelo.\n",
    "\n",
    "En el dataset MNIST hay 60.000 imágenes de entrenamiento y 10.000 de test. Las imágenes de test se encuentran estandarizadas para que diferentes investigadores puedan reportar sus resultados contra el mismo set de imágenes.\n",
    "\n",
    "Como no hay un set de validación predefinido vamos manualmente a dividir el set de 60.000 imágenes en un set de entrenamiento y otro de validación. Para ello vamos a separar 10.000 imágenes de forma aleatoria. Podemos lograr esto utilizando al función `random_split` includo en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante crear muestras a partir de una elección aleatoria ya que generalmente los datasets son creados de forma ordenada, de modo que las primeras imágenes corresponde a 0, luego a 1 y así sucesivamente. De modo que si escogemos el último 20% de las imágenes el set de validación correspondería a imágenes de 8 y 9s mientras que el set de entrenamiento no contendría a esas imagenes.\n",
    "\n",
    "Ahora podemos crear data loaders para ayudarnos a cargar data in muestras. En este caso vamos a usar muestras de un tamaño de 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa `shuffle=True` para el dataloader del conjunto de entrenamiento de modo de que las muestras generadas en cada epoca sean diferentes, además esta randomización nos permite generalizar y acelerar el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "\n",
    "Ahora que hemos preparado la data podemos definir un modelo.\n",
    "\n",
    "- Un modelo de regresión logística es casi identico a un modelo de regresión lineal, por ejemplo existen matrices de pesos y sesgo , y la salida es obtenida utilizando operaciones de matrices sencillas.\n",
    "- Para esto podemos utilizar `nn.Linear` para crear nuestro modelo en vez de definir nuestras matrices de forma manual.\n",
    "- Como `nn.Linear` espera que cada muestra de entrenamiento sea un vector cada imagen de  `1x28x28` debe ser transformado a un vector de tamaño 784 (28x28) antes de ser pasado al modelo.\n",
    "- La salida de cada imagen es un vector de tamaño 10, con cda elementos del vector indicando la probabilidad de que una imagen pertenezca a un label entre 0 a 9. El label predecido es simplemente aquel que tenga la probabilidad más alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Se crea el modelo de regresion logistica\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.8686e-02, -3.1765e-02,  1.0496e-02,  ..., -6.9599e-03,\n",
       "          5.5420e-05,  2.9746e-02],\n",
       "        [ 7.2859e-03, -9.5817e-03,  2.8749e-02,  ...,  2.1367e-02,\n",
       "         -3.0565e-02,  2.9453e-02],\n",
       "        [ 3.5430e-02, -5.0170e-03, -3.2109e-02,  ...,  2.5602e-02,\n",
       "         -1.2768e-02,  1.4677e-02],\n",
       "        ...,\n",
       "        [-7.2318e-03,  1.5741e-02,  3.3301e-02,  ..., -1.0405e-02,\n",
       "          2.4042e-02,  2.1889e-02],\n",
       "        [-5.1963e-03,  1.9359e-02,  2.3672e-02,  ..., -2.5725e-02,\n",
       "         -4.3051e-03,  5.9955e-03],\n",
       "        [-7.9952e-03, -7.6148e-03, -2.5388e-02,  ...,  2.0268e-02,\n",
       "         -3.3370e-02, -1.0982e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0199,  0.0316, -0.0341,  0.0176,  0.0182,  0.0015,  0.0138, -0.0043,\n",
       "        -0.0320,  0.0078], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver el modelo en total tiene 7850 parámetros (10 * 784 + 10), no obstante nada ha cambiado respecto a lo que hicimos previamente con el modelo de regresión lineal. Vamos a intentar generar algunas salidas usando nuestro modelo. Vamos a tomar las primeras 100 imagenes de la muestra de nuestro dataset y pasarselas al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 1, 2, 4, 3, 2, 8, 9, 6, 8, 0, 3, 2, 2, 4, 8, 2, 0, 1, 0, 9, 1, 1, 3,\n",
      "        7, 2, 1, 5, 7, 3, 1, 2, 9, 1, 9, 1, 7, 7, 8, 8, 2, 5, 2, 7, 1, 2, 5, 7,\n",
      "        5, 4, 7, 4, 3, 4, 1, 3, 6, 4, 8, 4, 6, 6, 4, 8, 8, 3, 7, 6, 9, 4, 3, 2,\n",
      "        4, 4, 6, 6, 7, 6, 5, 0, 7, 4, 7, 8, 5, 5, 5, 5, 7, 6, 5, 1, 8, 7, 0, 9,\n",
      "        7, 7, 5, 9, 4, 0, 6, 1, 6, 4, 9, 8, 1, 4, 7, 1, 9, 3, 2, 8, 5, 5, 3, 4,\n",
      "        1, 1, 6, 6, 2, 9, 7, 7])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3584 x 28], m2: [784 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-72eddc737460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [3584 x 28], m2: [784 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver se genera un error. Esto es debido a que la data de entrada no tiene la forma apropiada. Nuestras imagenes tienen una forma de `1x28x28` pero necesitamos que sean vectores de 784. Para esto vamos a utilizar el metodo `reshape` propio del tensor, el cual nos permite de forma eficiente ver cada imagen como un vector sin cambiar nada de la data.\n",
    "\n",
    "Para incluir esta funcionalidad adicional dentro de nuestro modelo necesitamos definir un modelo personalizado, estendiendo la clase `nn.Module` de Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1,784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del constructo se inicializan los pesos y sesgos usando `nn.Linear`. Y dentro del método `forward`, el cual es invocado cuando pasamos una muestra al modelo, se transforma la entrada a un vector y se le pasa al metodo self.linear.\n",
    "\n",
    "`xb.reshape(-1,28*28)` indica a Pytorch que queremos ver la entrada como un vector de 28x28.\n",
    "\n",
    "Notar que el modelo ya no tiene atributos `weight` o `bias` (ya que ahora se encuentran dentro del atributo `.linear`) no obstante ahora tiene un metodo `.parameters` que retorna una lista conteniendo los pesos y sesgo, y pueden ser utilizados por el optimizador de Pytoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0181, -0.0220, -0.0200,  ...,  0.0256,  0.0129,  0.0271],\n",
       "         [-0.0208,  0.0285, -0.0048,  ...,  0.0341, -0.0158, -0.0238],\n",
       "         [-0.0097, -0.0065,  0.0206,  ...,  0.0151,  0.0081,  0.0179],\n",
       "         ...,\n",
       "         [-0.0189,  0.0143,  0.0030,  ..., -0.0142,  0.0337, -0.0113],\n",
       "         [ 0.0069,  0.0107, -0.0328,  ...,  0.0095,  0.0151,  0.0303],\n",
       "         [-0.0349,  0.0041,  0.0010,  ...,  0.0111, -0.0066,  0.0190]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0280, -0.0306, -0.0144, -0.0250,  0.0242,  0.0208, -0.0016,  0.0142,\n",
       "         -0.0214,  0.0300], requires_grad=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro nuevo modelo personalizado puede ser usado exactamente igual que antes. Veamos como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape : torch.Size([128, 10])\n",
      "Sample outputs : \n",
      " tensor([[-0.5040, -0.1020,  0.1870,  0.0300, -0.1649,  0.0016, -0.1723, -0.3634,\n",
      "         -0.1981, -0.4008],\n",
      "        [-0.3832,  0.2775,  0.2560,  0.0249, -0.3063,  0.3165, -0.1059, -0.0885,\n",
      "         -0.2439, -0.2612]])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "print(f'outputs.shape : {outputs.shape}')\n",
    "print(f'Sample outputs : \\n {outputs[:2].data}')\n",
    "print(len(outputs.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver para cada imagen de entrada se obtiene una salida como un vector de tamaño 10, representando los 10 posibles digitos. Nos gustaría poder representar estas salidas como probabildiades, pero claramente los elementos no son probabilidades ya que podemos ver valores negativos o algunos mayores que 1.\n",
    "\n",
    "Para convertir las salidas en probabilidades vamos a usar la funcion de activacion softmax, que posee la siguiente fórmula:\n",
    "\n",
    "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
    "\n",
    "Primero se reemplazada cada valor `yi` en el vector de salida por `e^yi` para asegurarnos que los elementos sean positivos, luego se divide cada elemento en la suma de todos los elementos para asegurarnos que lleguen como maximo a 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función softmax se encuentra incluida en el paquete `torch.nn.functional` y requiere que especifiquemos la dimension sobre la cual se debe aplicar la funcion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades \n",
      ": tensor([[0.0701, 0.1048, 0.1399, 0.1195, 0.0984, 0.1162, 0.0977, 0.0807, 0.0952,\n",
      "         0.0777],\n",
      "        [0.0696, 0.1347, 0.1319, 0.1047, 0.0752, 0.1401, 0.0918, 0.0934, 0.0800,\n",
      "         0.0786]])\n",
      "Suma de probabilidades : 1.0\n"
     ]
    }
   ],
   "source": [
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "print(f'Probabilidades \\n: {probs[:2].data}')\n",
    "print(f\"Suma de probabilidades : {torch.sum(probs[0]).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos determinar el label predecido por cada imagen simplemente escogiendo el indice del elemento con la mayor probabilidad en cada output. Esto se puede lograr usando el metodo `torch.max`, el cual retorna el mayor elemento y el indice del mayor elemento sobre una dimension particular del tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 2, 2, 2, 2, 2, 2, 2, 3, 4, 2, 2, 1, 3, 2, 2, 9, 1, 5, 2, 5, 2, 3,\n",
      "        2, 2, 2, 5, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 1, 2, 1, 2, 5, 2, 2,\n",
      "        2, 2, 4, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 3, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2,\n",
      "        2, 2, 2, 2, 1, 2, 2, 8, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2,\n",
      "        2, 2, 5, 5, 1, 5, 2, 2, 2, 2, 2, 2, 3, 8, 3, 2, 2, 5, 2, 1, 2, 3, 1, 2,\n",
      "        2, 5, 2, 8, 2, 2, 2, 2])\n",
      "tensor([0.1399, 0.1401, 0.1423, 0.1526, 0.1203, 0.1325, 0.1548, 0.1347, 0.1268,\n",
      "        0.1291, 0.1285, 0.1727, 0.1833, 0.1414, 0.1335, 0.1441, 0.1374, 0.1214,\n",
      "        0.1416, 0.1357, 0.1691, 0.1430, 0.1761, 0.1281, 0.1409, 0.1223, 0.1674,\n",
      "        0.1454, 0.1289, 0.1485, 0.1634, 0.1296, 0.1451, 0.1354, 0.1452, 0.1518,\n",
      "        0.1277, 0.1422, 0.1207, 0.1337, 0.1222, 0.1233, 0.1510, 0.1443, 0.1857,\n",
      "        0.1192, 0.1535, 0.1318, 0.1328, 0.1329, 0.1242, 0.1264, 0.1345, 0.1373,\n",
      "        0.1329, 0.1189, 0.1171, 0.1374, 0.1494, 0.1452, 0.1425, 0.1402, 0.1116,\n",
      "        0.1493, 0.1376, 0.1260, 0.1357, 0.1596, 0.1260, 0.1429, 0.1285, 0.1187,\n",
      "        0.1456, 0.1259, 0.1347, 0.1468, 0.1318, 0.1375, 0.1343, 0.1170, 0.1618,\n",
      "        0.1199, 0.1294, 0.1475, 0.1292, 0.2203, 0.1173, 0.1604, 0.1707, 0.1399,\n",
      "        0.1506, 0.1331, 0.1291, 0.1353, 0.1485, 0.1311, 0.1519, 0.1242, 0.1420,\n",
      "        0.1344, 0.1277, 0.1470, 0.1436, 0.1384, 0.1390, 0.1706, 0.1193, 0.1309,\n",
      "        0.1324, 0.1285, 0.1394, 0.1759, 0.1624, 0.1219, 0.1406, 0.1252, 0.1405,\n",
      "        0.1295, 0.1246, 0.1560, 0.1403, 0.1273, 0.1417, 0.1234, 0.1622, 0.1556,\n",
      "        0.1837, 0.1475], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los numeros mostrados son los labels predecidos. Comparemoslos con los labels reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 8, 7, 1, 9, 2, 5, 1, 1, 3, 5, 8, 3, 4, 8, 1, 9, 3, 0, 8, 3, 5, 9,\n",
       "        1, 1, 0, 8, 4, 5, 3, 6, 8, 7, 7, 7, 8, 9, 2, 8, 7, 4, 6, 3, 0, 1, 3, 5,\n",
       "        2, 7, 1, 1, 5, 2, 5, 3, 1, 6, 9, 4, 7, 5, 1, 0, 8, 5, 6, 7, 8, 7, 0, 1,\n",
       "        0, 3, 9, 2, 6, 2, 9, 0, 0, 1, 9, 6, 8, 0, 1, 3, 8, 7, 5, 6, 3, 1, 7, 1,\n",
       "        0, 1, 6, 8, 5, 7, 2, 2, 0, 5, 3, 4, 4, 2, 0, 6, 3, 8, 0, 7, 8, 9, 4, 0,\n",
       "        8, 0, 7, 2, 9, 8, 5, 6])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente los labels predecidos y los reales son completamente diferentes. Obviamente esto es porque los pesos se han inicialiado de forma aleatoria al igual que el sesgo. Ahora necesitamos entrenar el modelo para obtener mejores predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrica de evaluacion y funcion de perdida\n",
    "\n",
    "Necesitamos una forma de valuar que tan bien esta nuestro modelo prediciendo. Una forma natural de hacer esto seria encontrar el procentaje de labels que ha predecido correctamente, es decir la presisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El operador `==` funciona como una comparasión de lementos entre dos tensores de igual forma, y retorna un nuevo tensor conteniedo 0s para los elementos no iguales y 1s para los elementos iguales. Pasarle esto al metodo `torch.sum` retorna la cantidad de labels predecidos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0703)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras que la presisión es una buena forma para nosotros los humanos de evaluar el modelo, no puede ser usado como función de perdida para optimizar nuestro modelo por las siguientes razones.\n",
    "- No es una función diferenciable. `torc.hmax` y `==` son ambas operaciones no  continuas y no diferenciables\n",
    "- No toma en cuenta la probabilidad actual predicha por el modelo, asi que no puede aportar un feedback suficiente para mejorar.\n",
    "\n",
    "Una funcion de perdida comunmente usada para problemas de clasificación es la **entropía cruzada**, la cual tiene la siguiente formula:\n",
    "\n",
    "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
    "\n",
    "Aún si parece complicado la verdad es bastante sencillo.\n",
    "\n",
    "- Por cada fila de los outputs se escoge la probabilidad predicha para el label correcto.\n",
    "- Luego toma el logaritmo natural de la probabilidad escogida. Si la probabilidad es alta (cercana a 1), entonces el logaritmo es un valor negativo pequeño cercano a 0. Y si la probabilidad es pequeña (cercana a 0) entonces el logaritmo es un numero negativo muy alto. Luego multiplicamos el resultado por -1 lo que nos da un valor alto de la función de perdida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3209, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(outputs,labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenado el modelo\n",
    "\n",
    "Ahora que se ha definido los data_loeaders, el modelo y la funcion de perdida estamos listos para entrenar el modelo. El algoritmo de entrenamiento es identico al de regresión lineal, con la adición de una fase de validación, tal y como se ve en el siguiente pseudocodigo.\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    for batch in train_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Compute gradients\n",
    "        # Update weights\n",
    "        # Reset gradients\n",
    "    \n",
    "    # Validation phase\n",
    "    for batch in val_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Calculate metrics (accuracy etc.)\n",
    "    # Calculate average validation loss & metrics\n",
    "    \n",
    "    # Log epoch, loss & metrics for inspection\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generar predicciones\n",
    "        loss = F.cross_entropy(out, labels) # Calcular perdida\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generar predicciones\n",
    "        loss = F.cross_entropy(out, labels)   # Calcular perdida\n",
    "        acc = accuracy(out, labels)           # Calcular precision\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combinar perdidas\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combinar precision\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos una funcion `evaluate` para evaluar, la cual generara la fase de validación y una funcion `fit` para generar el proceso completo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Fase entrenamiento\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Fase validacion\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funcion`fit` guarda la funcion de perdida y la precision de cada epoca y retorna un historico del prceso de entrenamiento. Es una forma util para visualizar el proceso de entrenamiento. Antes de entrenar el modelo veamos como es la performance en el set de validaciónm con pesos y sesgos inicializados aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3183751106262207, 'val_acc': 0.07446598261594772}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precision inicial es cercana al 10%, lo cual tiene sentido ya que son pesos aleatorios para una muestra de 10 posibles salidas. Probabilidad de 1 en 10 de acertar de forma aleatoria.\n",
    "\n",
    "Ahora entrenemos el modelo por 5 epocas y veamos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9531, val_acc: 0.5808\n",
      "Epoch [1], val_loss: 1.6826, val_acc: 0.7182\n",
      "Epoch [2], val_loss: 1.4804, val_acc: 0.7612\n",
      "Epoch [3], val_loss: 1.3278, val_acc: 0.7826\n",
      "Epoch [4], val_loss: 1.2105, val_acc: 0.7961\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En solo 5 epocas se ha obtenido una precision cercana al 80%. Ahora veamos como mejora el modelo con algunas epocas mas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.1183, val_acc: 0.8062\n",
      "Epoch [1], val_loss: 1.0444, val_acc: 0.8159\n",
      "Epoch [2], val_loss: 0.9841, val_acc: 0.8215\n",
      "Epoch [3], val_loss: 0.9339, val_acc: 0.8261\n",
      "Epoch [4], val_loss: 0.8915, val_acc: 0.8303\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.8552, val_acc: 0.8329\n",
      "Epoch [1], val_loss: 0.8237, val_acc: 0.8369\n",
      "Epoch [2], val_loss: 0.7963, val_acc: 0.8390\n",
      "Epoch [3], val_loss: 0.7720, val_acc: 0.8419\n",
      "Epoch [4], val_loss: 0.7504, val_acc: 0.8439\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7310, val_acc: 0.8459\n",
      "Epoch [1], val_loss: 0.7136, val_acc: 0.8488\n",
      "Epoch [2], val_loss: 0.6978, val_acc: 0.8510\n",
      "Epoch [3], val_loss: 0.6833, val_acc: 0.8520\n",
      "Epoch [4], val_loss: 0.6702, val_acc: 0.8532\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras la precision crece a medida que entrenamos el modelo, es claro que la mejoria en cada epoca es cada vez menor. Podemos verlo a traves de una grafica en matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epocas')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Precision vs numero epocas');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testear el modelo con imagenes conocidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(\n",
    "    root='data/',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`img.unsqueeze` agrega otra dimension al comienzo del tensor 1x28x28, haciendolo uno de 1x1x28x28, de este modo el modelo ve la imagen como una muestra que contiene solo una iamgen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una funcion `predict_image` que nos permite obtener el label predecido por nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 , Predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 , Predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9 , Predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[193]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 , Predicted: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[1839]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6431774497032166, 'val_acc': 0.8594726324081421}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
